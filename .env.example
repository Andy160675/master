# ==============================
# CONNECTION ENDPOINTS (Distributed Deployment)
# ==============================
# These variables decouple services from localhost so the Sovereign stack can run across
# NAS, Tailscale nodes, and "Sovereign Process Isolation" boundaries.

# Truth Engine (FastAPI)
# Example (local): TRUTH_ENGINE_URL=http://localhost:8000
# Example (NAS/LAN): TRUTH_ENGINE_URL=http://192.168.4.114:8000
TRUTH_ENGINE_URL=http://localhost:8000

# Truth Engine CORS origins
# "*" allows all origins (default). For tighter control, set comma-separated origins.
# Example: TRUTH_ENGINE_CORS_ORIGINS=http://localhost:5173,http://192.168.4.114:5173
TRUTH_ENGINE_CORS_ORIGINS=*

# Ollama host (for local LLM, OpenAI-compatible endpoint is typically ${OLLAMA_HOST}/v1)
# Example: OLLAMA_HOST=http://localhost:11434
# Example (remote): OLLAMA_HOST=http://192.168.4.114:11434
OLLAMA_HOST=http://localhost:11434

# Optional: override full OpenAI-compatible base URL explicitly
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_BASE_URL=http://192.168.4.114:11434/v1
# OLLAMA_BASE_URL=
